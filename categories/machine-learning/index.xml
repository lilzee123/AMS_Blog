<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Machine Learning on ams</title>
        <link>https://lilzee123.github.io/AMS_Blog/categories/machine-learning/</link>
        <description>Recent content in Machine Learning on ams</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Ovo</copyright>
        <lastBuildDate>Thu, 08 Jan 2026 18:39:21 +0800</lastBuildDate><atom:link href="https://lilzee123.github.io/AMS_Blog/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Diffusion</title>
        <link>https://lilzee123.github.io/AMS_Blog/p/diffusion/</link>
        <pubDate>Thu, 08 Jan 2026 18:39:21 +0800</pubDate>
        
        <guid>https://lilzee123.github.io/AMS_Blog/p/diffusion/</guid>
        <description>&lt;img src="https://lilzee123.github.io/AMS_Blog/p/diffusion/Haruka.png" alt="Featured image of post Diffusion" /&gt;&lt;h2 id=&#34;1-ddpm&#34;&gt;1. DDPM
&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;前向扩散：对一张图片不断加入噪声（正态分布），产生一系列噪声数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;去噪过程：训练一个噪声predicter，预测每一个噪声图片的噪声。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Diffusion的成功：N次迭代？&lt;/p&gt;
&lt;p&gt;噪声predicter：输入为噪声图片，时间步t，text向量（Text-to-Image）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-14.png&#34;
	width=&#34;829&#34;
	height=&#34;235&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-14_hu_2e225eea30313f03.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-14_hu_3317af237bf0864e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;352&#34;
		data-flex-basis=&#34;846px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;评价指标：&lt;/p&gt;
&lt;p&gt;FID：Frechet distance between the two Gaussians，评价图片逼真程度&lt;/p&gt;
&lt;p&gt;CLIP Score：评价Text-to-Image的好坏&lt;/p&gt;
&lt;p&gt;Stable Diffusion（常用框架）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-9.png&#34;
	width=&#34;936&#34;
	height=&#34;428&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-9_hu_6f94b4b358480831.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-9_hu_17ac036f2956c1d9.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;218&#34;
		data-flex-basis=&#34;524px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-11.png&#34;
	width=&#34;830&#34;
	height=&#34;402&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-11_hu_26b259ea0a2fd20e.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-11_hu_39a08ceb02c382f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;495px&#34;
	
&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Decoder&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;（1）中间产物是图片的压缩版本，训练：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-8.png&#34;
	width=&#34;483&#34;
	height=&#34;140&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-8_hu_e14112f70a83c24e.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-8_hu_85c3da7bfb52eb9c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;345&#34;
		data-flex-basis=&#34;828px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;（2）中间产物是latent representation，训练auto-encoder：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-7.png&#34;
	width=&#34;949&#34;
	height=&#34;295&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-7_hu_4d2c991431918eb.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-7_hu_2fce253d3cd76674.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;321&#34;
		data-flex-basis=&#34;772px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generation Model&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将图片输入encoder，得到latent representation，对latent representation进行加噪和去噪操作进行训练，预测过程不变。（因为输出变成了一个latent representation）&lt;/p&gt;
&lt;h2 id=&#34;2-details&#34;&gt;2. Details
&lt;/h2&gt;&lt;h3 id=&#34;21-training-algorithm训练过程&#34;&gt;2.1 Training Algorithm（训练过程）
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;抽取clean image，原始图片&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;抽取一个时间步t&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;随机取样noise&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;计算梯度：原始图片和noise以一个权重（事先定义）相加，经过predicter预测一个noise，与原noise做平方差，求导得梯度。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-13.png&#34;
	width=&#34;758&#34;
	height=&#34;386&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-13_hu_9fc78ade9637443b.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-13_hu_dd675ff5d3b883e9.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;196&#34;
		data-flex-basis=&#34;471px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;数学推导：&lt;/p&gt;
$$x_{0}到x_{t}可以简化为如下公式，因此可以把beta式简化为alpha_{t},alpha_{t}由大变小$$&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-12.png&#34;
	width=&#34;981&#34;
	height=&#34;551&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-12_hu_68d94e240805c4ee.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-12_hu_e37fcd396f1cbbc8.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;178&#34;
		data-flex-basis=&#34;427px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;22-sampling-algorithm生成过程&#34;&gt;2.2 Sampling Algorithm（生成过程）
&lt;/h3&gt;&lt;p&gt;公式推导（已知）：&amp;ndash;基于最大似然估计&lt;/p&gt;
&lt;p&gt;已知噪声预测$q(x_{t-1}|x_{t},x_0)$的分布（其实就是 $x_{t-1}$的分布），而predicter预测结果 $P(x_{t-1}|x_{t})$的分布要与噪声预测 $q(x_{t-1}|x_{t},x_0)$分布越接近越好，在不考虑方差的情况下，那就是使得predicter预测的均值与噪声预测分布的均值越接近越好。&lt;/p&gt;
&lt;p&gt;噪声预测均值为:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image.png&#34;
	width=&#34;248&#34;
	height=&#34;110&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image_hu_7585ce55e1b585c4.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image_hu_43cbb3921b2cbcf9.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;225&#34;
		data-flex-basis=&#34;541px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;即predicter预测分布的均值，再加上随机噪声z和固定方差 $\sigma$( $\sigma_t^2通常取\beta_t$)，得到预测分布。&lt;/p&gt;
&lt;p&gt;补充：在得到 $x_0$（利用预测噪声）和 $x_t$之后，可以利用后验概率计算得到 $x_{t-1}$(在数学上和上述式子是等价的):&lt;/p&gt;
$$\hat{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(x_t - \sqrt{1-\bar{\alpha}_t} \epsilon_\theta)$$$$\tilde{mu}_t = \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1-\bar{\alpha}_t}x_0 + \frac{\sqrt{\alpha_t}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_t}x_t$$&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-10.png&#34;
	width=&#34;675&#34;
	height=&#34;257&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-10_hu_d0169064ae54dee1.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-10_hu_d0ed5259126b503c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;262&#34;
		data-flex-basis=&#34;630px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-1.png&#34;
	width=&#34;984&#34;
	height=&#34;301&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-1_hu_db123642e31f8a12.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-1_hu_f02e887ff5ffd056.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;326&#34;
		data-flex-basis=&#34;784px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;3-improved-ddpm&#34;&gt;3. Improved DDPM
&lt;/h2&gt;&lt;p&gt;虽然 DDPM 在生成任务上取得了不错的效果，但如果使用一些 metric 对 DDPM 进行评价，就会发现其虽然能在 FID 和 Inception Score 上获得不错的效果，但在&lt;strong&gt;负对数似然（Negative Log-likelihood，NLL）&lt;/strong&gt;这个指标上表现不够好。&lt;/p&gt;
&lt;p&gt;NLL 上的表现体现的是模型捕捉数据整体分布的能力。而且有工作表明即使在 NLL 指标上仅有微小的提升，就会在生成效果和特征表征能力上有很大的提升。&lt;/p&gt;
&lt;p&gt;Improved DDPM 主要是针对 DDPM 的训练过程进行改进，主要从两个方面进行改进：&lt;/p&gt;
&lt;p&gt;1.不使用 DDPM 原有的固定方差，而是使用可学习的方差；&lt;/p&gt;
&lt;p&gt;这个工作的作者认为因为方差的变化范围比较小，不太容易用神经网络进行学习，所以实际上使用的方差是对  $\beta_t$和 $\tilde{\beta_t}$ 进行插值的结果（log形式，v是一个可学习的参数（0-1））， $\beta_t$是方差最大值， &lt;/p&gt;
$$\tilde{\beta}_t = \frac{1-\bar{\alpha}_{t-1}}{1-\bar{\alpha}_t} \beta_t $$&lt;p&gt; 是最小值&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-2.png&#34;
	width=&#34;356&#34;
	height=&#34;50&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-2_hu_9002ef57dbf0c236.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-2_hu_54136d0e44ca8c54.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;712&#34;
		data-flex-basis=&#34;1708px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;2.改进了加噪过程，使用余弦形式的 Scheduler，而不是线性 Scheduler。&lt;/p&gt;
&lt;p&gt;本文的作者发现线性的  $\beta_t$对于高分辨率图像效果不错，但对于低分辨率的图像表现不佳。因为如果最开始的时候加入很大的噪声，会严重破坏图像信息，不利于图像的学习。&lt;/p&gt;
&lt;p&gt;作者把方差用一种 cosine 的形式定义，不过并不是直接定义 $\beta_t$，而是定义 $\bar{\alpha_t}$。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-3.png&#34;
	width=&#34;128&#34;
	height=&#34;34&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-3_hu_4f9e0c4610faaed5.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-3_hu_a410911f0b706bba.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;376&#34;
		data-flex-basis=&#34;903px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;变化差异如图所示。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-4.png&#34;
	width=&#34;383&#34;
	height=&#34;79&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-4_hu_bce26153df3bf976.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-4_hu_fcc878c785ff9567.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;484&#34;
		data-flex-basis=&#34;1163px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-5.png&#34;
	width=&#34;1280&#34;
	height=&#34;869&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-5_hu_daf9393d09a11482.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-5_hu_3f5768851434684.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;训练过程和代码实现详见：https://littlenyima.github.io/posts/15-improved-denoising-diffusion-probabilistic-models/&lt;/p&gt;
&lt;p&gt;3.重要性采样&lt;/p&gt;
&lt;p&gt;在计算损失函数时，损失函数包括DDPM的L2损失和VLB损失两项。其中VLB损失为1-T的所有时间步的加和平均。&lt;/p&gt;
&lt;p&gt;但这会带来两个问题：&lt;/p&gt;
&lt;p&gt;1.&lt;strong&gt;计算量大：&lt;/strong&gt; 需要计算所有 T 个时间步的损失。&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;方差高：&lt;/strong&gt; 不同时间步 t 的 $L_t$ 项对方差的贡献可能非常不均匀。&lt;/p&gt;
&lt;p&gt;为了解决这两个问题，Improved Diffusion 模型提出了一种&lt;strong&gt;采样&lt;/strong&gt;时间步 t 的方法来估计 VLB 损失，具体为对每个时间步存储最近的10个历史损失，根据每个时间步的损失大小分配权重w，并且对w进行动态更新（EMA？）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;实际操作：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1.&lt;strong&gt;采样 t：&lt;/strong&gt; 模型从重要性采样分布 p(t) 中&lt;strong&gt;采样&lt;/strong&gt;一个时间步 t&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;计算 &lt;/strong&gt;$L_t$&lt;strong&gt;：&lt;/strong&gt; 计算在当前采样的 t 上的 VLB 损失项 &lt;/p&gt;
$$L_t = D_{\text{KL}}(q(\mathbf{x}_{t-1}|\mathbf{x}_t, \mathbf{x}_0) || p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t))$$&lt;p&gt;3.&lt;strong&gt;应用权重：&lt;/strong&gt; 将 $L_t$ 乘以重要性校正权重 $W_t = frac{1/T}{p(t)}$，得到该迭代的 VLB 估计损失： &lt;/p&gt;
$$\hat{L}_{\text{VLB}} = W_t \cdot L_t$$&lt;p&gt;最终的训练损失是这两个损失的加权和：&lt;/p&gt;
$$\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{simple}} + \lambda \cdot \mathcal{L}_{\text{VLB}}$$&lt;p&gt;其中 $\lambda$ 是 VLB 损失的权重，通常是一个较小的常数。&lt;/p&gt;
&lt;p&gt;优化 VLB 损失直接强制模型学习一个更准确的噪声概率分布 $p_\theta(\mathbf{x}_0)$（因为VLB损失使得模型预测方差更接近真实的方差）。这使得模型能够更好地泛化到训练集之外的数据&lt;/p&gt;
&lt;p&gt;为了计算简单，加快优化，对 VLB 损失中的均值项进行了 stop-gradient，从而让  $\mathcal{L}_{\text{simple}}$依然是均值的主要决定因素。&lt;/p&gt;
&lt;p&gt;IDDPM 的训练使用了两个主要目标：&lt;/p&gt;
&lt;p&gt;1.&lt;strong&gt;$\mathcal{L}_{\text{simple}}$：&lt;/strong&gt; 主要负责优化 &lt;strong&gt;去噪均值（即 &lt;/strong&gt;$\mathbf{\epsilon}_\theta$&lt;strong&gt;）&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;2.&lt;strong&gt;$\mathcal{L}_{\text{VLB}}$：&lt;/strong&gt; 主要负责优化 &lt;strong&gt;反向过程的方差 &lt;/strong&gt;&lt;/p&gt;
$$\mathbf{\Sigma}_\theta$$&lt;p&gt;（因为均值部分已经被 $\mathcal{L}_{\text{simple}}$ 充分优化）。&lt;/p&gt;
&lt;p&gt;KL散度计算（注意，模型预测的方差实际上是方差的log形式）：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-6.png&#34;
	width=&#34;578&#34;
	height=&#34;176&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-6_hu_d4c43fafcce0591a.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-6_hu_208703a774851b79.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;328&#34;
		data-flex-basis=&#34;788px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;4-classifier-free-guidance&#34;&gt;4. Classifier Free Guidance
&lt;/h2&gt;&lt;p&gt;通过加入condition，来引导生成的图像朝着我们想要的方向进行。&lt;/p&gt;
&lt;p&gt;中间计算过程不再推导，基于Classifier Guidance实现&lt;/p&gt;
&lt;p&gt;如下所示。两项分别是无条件生成的分数以及从无条件生成指向有条件生成的方向，这样看就更清晰了：classifier-free guidance 就是从无条件生成的基础上向某个条件的方向移动， s是一个用来控制条件重要性的参数。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-15.png&#34;
	width=&#34;585&#34;
	height=&#34;70&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-15_hu_a6a6e8fda5753dbb.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-15_hu_f00bb63b34c3b407.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;835&#34;
		data-flex-basis=&#34;2005px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;可以通过分别对无条件生成和有条件生成进行学习，得到无需分类器的有条件生成模型。(无条件生成是有条件生成的基础，生成的质量和多样性是由无条件生成的分数保证的，如果只有有条件生成而没有无条件生成，那么生成效果可能不佳。)&lt;/p&gt;
&lt;p&gt;为了联合训练有条件和无条件的情况，在训练时需要以一定的概率p 将条件输入替换为空 。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-16.png&#34;
	width=&#34;1280&#34;
	height=&#34;416&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-16_hu_4278b480eeda0054.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-16_hu_d21b3027de99c318.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;307&#34;
		data-flex-basis=&#34;738px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;文中也给出了采样算法的流程，可以看到预测噪声由无条件和有条件两部分加权得到：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-17.png&#34;
	width=&#34;1280&#34;
	height=&#34;527&#34;
	srcset=&#34;https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-17_hu_8838a1749aaaf3cd.png 480w, https://lilzee123.github.io/AMS_Blog/p/diffusion/images/image-17_hu_f2d7c1331f8fbd26.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;242&#34;
		data-flex-basis=&#34;582px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;条件注入的方法&#34;&gt;条件注入的方法
&lt;/h3&gt;&lt;p&gt;我们知道去噪模型通常使用的都是 UNet，将条件注入 UNet 有几种比较常见的方式，即交叉注意力、通道注意力以及自适应归一化等。&lt;/p&gt;
&lt;p&gt;1.Cross Attention *&lt;/p&gt;
&lt;p&gt;交叉注意力是比较常用的一种条件注入方式，在注入时是以 x 为 query、以 condition 为 key 和 value。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://zhuanlan.zhihu.com/p/1970452052778919816&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://zhuanlan.zhihu.com/p/1970452052778919816&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2.Channel-wise Attention *&lt;/p&gt;
&lt;p&gt;通道注意力相对比较简单。虽然叫注意力，但其实就是做完 projection 直接加到 time embedding 上。&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.cnblogs.com/medcs/p/18926383&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.cnblogs.com/medcs/p/18926383&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;TimestepEmbedding&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Module&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;k&#34;&gt;def&lt;/span&gt; &lt;span class=&#34;nf&#34;&gt;forward&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;condition&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;condition&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;c1&#34;&gt;# 条件在这里注入&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cond_proj&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;condition&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linear_1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;act&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;act&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linear_2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;k&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;post_act&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;           &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;post_act&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;       &lt;span class=&#34;k&#34;&gt;return&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sample&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;上述代码中的 &lt;code&gt;cond_proj&lt;/code&gt; 的定义如下，可以看到就是一个 linear 层：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;bp&#34;&gt;self&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cond_proj&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nn&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Linear&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cond_proj_dim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;in_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bias&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        
    </channel>
</rss>
